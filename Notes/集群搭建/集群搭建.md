## 集群搭建步骤

1. 创建虚拟机

   * 虚拟机配置

     * ```sh
       /boot 1g 
       /swap 4g 
       / 45g
       ```

       ![image-20210410161356535](C:\Users\77023\Desktop\md\集群搭建\images\image-20210410161356535.png)

2. 系统设置

   * 关闭防火墙

     * ```sh
       #关闭防火墙
       #service iptables stop
       systemctl stop firewalld
       #关闭开机自动启用
       #chkconfig iptables off
       systemctl disable firewalld.service
       ```

   * 设置虚拟机IP
     
   * ![image-20210410162954649](C:\Users\77023\Desktop\md\集群搭建\images\image-20210410162954649.png)
     
   * 设置Windows以太网VMnet8的属性

     * ![image-20210410163201901](C:\Users\77023\Desktop\md\集群搭建\images\image-20210410163201901.png)

     * ![image-20210410163436509](C:\Users\77023\Desktop\md\集群搭建\images\image-20210410163436509.png)

   * 设置网络参数

     * ```sh
       #设置网络信息
       vim /etc/sysconfig/network-scripts/ifcfg-ens33
       
       TYPE="Ethernet"
       PROXY_METHOD="none"
       BROWSER_ONLY="no"
       #设置为静态IP地址
       BOOTPROTO="static"
       DEFROUTE="yes"
       IPV4_FAILURE_FATAL="no"
       IPV6INIT="yes"
       IPV6_AUTOCONF="yes"
       IPV6_DEFROUTE="yes"
       IPV6_FAILURE_FATAL="no"
       IPV6_ADDR_GEN_MODE="stable-privacy"
       NAME="ens33"
       UUID="3278f4c6-a17b-430f-b1f4-775d891a97da"
       DEVICE="ens33"
       ONBOOT="yes"
       #设置静态IP地址
       IPADDR=192.168.131.100
       #设置网关,参考虚拟机
       GATEWAY=192.168.131.2
       #设置dns,参考虚拟机
       DNS1=192.168.131.2
       
       注:
       #如果发生IP地址变化,ens33丢失
       #关闭NetworkManager
       service NetworkManager stop
       chkconfig NetworkManager off
       #重启网络
       systemctl start network.service
       ```

   * 设置主机名

     * ```sh
       vim /etc/hostname
       #设置主机名
       hadoop100
       ```

   * 设置集群主机与IP地址映射

     * ```sh
       vim /etc/hosts
       #集群主机名IP映射
       192.168.131.100   hadoop100
       192.168.131.101   hadoop101
       192.168.131.102   hadoop102
       192.168.131.103   hadoop103
       192.168.131.104   hadoop104
       192.168.131.105   hadoop105
       192.168.131.106   hadoop106
       192.168.131.107   hadoop107
       192.168.131.108   hadoop108
       
       #同时设置windows设置集群主机与IP地址映射
       C:\Windows\System32\drivers\etc\hosts
       
       192.168.131.100   hadoop100
       192.168.131.101   hadoop101
       192.168.131.102   hadoop102
       192.168.131.103   hadoop103
       192.168.131.104   hadoop104
       192.168.131.105   hadoop105
       192.168.131.106   hadoop106
       192.168.131.107   hadoop107
       192.168.131.108   hadoop108
       ```

     * 

3. 添加用户并授权root权限

   ```sh
   #添加用户
   useradd hadoop100
   passwd hadoop100
   
   #授权root
   vim /etc/sudoers
   
   %wheel  ALL=(ALL)       ALL
   hadoop100       ALL=(ALL)       NOPASSWD: ALL
   
   注:
   #临时使用root
   su root
   
   #退出root
   exit
   ```
   
4. 安装软件(JDK)

   ```sh
   #解压软件包到指定目录
   tar -zxvf jdk-8u212-linux-x64.tar.gz -C /opt/module/
   
   #配置环境变量
   #创建自定义环境变量文件
   vim /etc/profile.d/my_env.sh
   #配置环境变量到my_env.sh
   #JAVA_HOME
   export JAVA_HOME=/opt/module/jdk1.8.0_212
   export PATH=$PATH:$JAVA_HOME/bin
   
   #环境变量配置生效
   source /etc/profile
   ```

5. 集群拷贝

    scp命令

   ```sh
   #完全复制,速度慢,可以双方远程复制
   #scp -递归 数据源 数据先
   scp -r root@hadoop102:/opt/module/* root@hadoop103:/opt/module/
   ```

   rsync命令

   ```sh
   #差异复制,速度快,只能单向远程复制
   #rsync -av /opt/module/* root@hadoop103:/opt/module/
   ```

   xsync集群分发脚本

   ```sh
   #创建脚本文件
   vim /bin/xsync
   #添加执行权
   chmod +x xsync
   
   #使用
   xsync /xxx
   ```

   ```sh
   #脚本内容
   #!/bin/bash
   #1. 判断参数个数
   if [ $# -lt 1 ]
   then
       echo Not Enough Arguement!
       exit;
   fi
   #2. 遍历集群所有机器
   for host in hadoop102 hadoop103 hadoop104
   do
   	echo ==================== $host ====================
   	#3. 遍历所有目录，挨个发送
   	for file in $@
   	do
   		#4. 判断文件是否存在
   		if [ -e $file ]
   		then
   			#5. 获取父目录
   			pdir=$(cd -P $(dirname $file); pwd)
   			#6. 获取当前文件的名称
   			fname=$(basename $file)
   			ssh $host "mkdir -p $pdir"
   			rsync -av $pdir/$fname $host:$pdir
   		else
   			echo $file does not exists!
   		fi
   	done
   done
   ```

6. 免密登录

   ```sh
   #进入home目录
   ssh-keygen -t rsa
   
   #复制公钥到目标机,包括自己
   ssh-copy-id hadoop102
   ssh-copy-id hadoop103
   ssh-copy-id hadoop104
   
   #登录退出目标机
   ssh hadoop102
   exit
   ```


7. 大数据框架常用端口

   ```xml
   <!-- 指定hive存储元数据要连接的地址 -->
   <property>
       <name>hive.metastore.uris</name>
       <value>thrift://hadoop102:9083</value>
   </property>
   ```

   

8. 解决shell文件格式错误

   ```sh
   #安装插件
   yum install -y dos2unix
   #执行转换
   dos2unix hiveservice.sh 
   ```

   